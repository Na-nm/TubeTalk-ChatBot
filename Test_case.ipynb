{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f295c1",
   "metadata": {},
   "source": [
    "# TubeTalk Test cases  \n",
    "\n",
    "This notebook represents the initial version of the project. If you’d like to test the code without using the Chainlit interface, you can run this notebook locally and enter a YouTube link as input. \n",
    "\n",
    "In the output sections, you’ll find the results based on our test and we performed it on a short video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179917ca",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ab7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import uuid\n",
    "import tempfile\n",
    "import asyncio\n",
    "from io import BytesIO\n",
    "import whisper\n",
    "import yt_dlp\n",
    "import re \n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "import chainlit as cl\n",
    "from chainlit.element import Element\n",
    "import numpy as np\n",
    "import soundfile as sf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YouTube Audio Download and Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a89a93d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "import os\n",
    "\n",
    "# Download and convert YouTube audio to mp3\n",
    "def download_audio(youtube_url, filename=\"./audio.mp3\"):\n",
    "    print(\"Starting download...\")\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'outtmpl': filename,\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'mp3',\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "        'quiet': True\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([youtube_url])\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        print(f\"Downloaded succesfully: {filename}\")\n",
    "    else:\n",
    "        print(f\"Downloaded failed: {filename}\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858778f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "# Convert mp3 to wav\n",
    "def transcribe_audio(audio_path):\n",
    "    print(\"Starting transcription...\")\n",
    "    model = whisper.load_model(\"base\") \n",
    "    result = model.transcribe(audio_path)\n",
    "    text = result['text']\n",
    "    print(\"Transcription completed.\")\n",
    "\n",
    "    # Create a text file with the transcribed text\n",
    "    text_filename = \"transcribed_text.txt\"\n",
    "\n",
    "    # Open the file in write mode and save the transcribed text\n",
    "    with open(text_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "\n",
    "    print(f\"Transcribed text has been saved successfully to {text_filename}.\")\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadba1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download...\n",
      "Downloaded succesfully: ./audio.mp3\n",
      "Starting transcription...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nada\\Documents\\AI Engineering bootcamp\\final project\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription completed.\n",
      "\n",
      "Transcribed Text:\n",
      "===================================\n",
      " In this video, we're going to be talking about the promptub. The promptub is a feature in Lensmith that allows us to save and version prompts that we've been iterating on and refining. Once we've saved our prompt, we can then also pull it locally into our application and reuse it in our code. To show you how to do this, we're going to continue with our example with our parent named Poly. As a reminder, Poly is a parent and Poly has some facts that she can use to answer user's questions about he\n"
     ]
    }
   ],
   "source": [
    "# Testing the functions\n",
    "youtube_url = input(\"Enter the youtube URL: \").strip()\n",
    "\n",
    "audio_file = download_audio(youtube_url)\n",
    "\n",
    "if os.path.exists(audio_file):\n",
    "    text = transcribe_audio(audio_file)\n",
    "else:\n",
    "    print(\"Failed to download audio file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9f84eb",
   "metadata": {},
   "source": [
    "## Tokenization and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be46331d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed text has been loaded successfully from transcribed_text.txt.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reading the transcribed text from the file\n",
    "text_filename = \"transcribed_text.txt\"\n",
    "with open(text_filename, \"r\", encoding=\"utf-8\") as file:\n",
    "    loaded_text = file.read()\n",
    "\n",
    "print(f\"Transcribed text has been loaded successfully from {text_filename}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb74b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Tokenize and chunk the text using HuggingFace tokenizer\n",
    "def tokenize_and_chunk_hf(text, max_tokens=256):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    return [tokenizer.decode(tokens[i:i + max_tokens]).strip() for i in range(0, len(tokens), max_tokens)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44869ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text has been smartly split into 2 chunks successfully.\n",
      "Total smart chunks: 2\n",
      "All chunks preview:\n",
      "==================================================\n",
      "Chunk 1:\n",
      "In this video, we're going to be talking about the promptub. The promptub is a feature in Lensmith that allows us to save and version prompts that we've been iterating on and refining. Once we've saved our prompt, we can then also pull it locally into our application and reuse it in our code. To show you how to do this, we're going to continue with our example with our parent named Poly. As a reminder, Poly is a parent and Poly has some facts that she can use to answer user's questions about herself. Let's pretend that we've iterated on this prompt for some time now and we're pretty happy with it and ready to save it for reuse. It's probably not a good idea to save this prompt with these inputs hard-coded in and also with this question from the user hard-coded. What we can do is we can replace these facts with an input variable. This essentially allows the user to, at runtime, pass in the values that they want to use for both the facts that Poly has access to and the question that the human asks. Cool. Now let's go ahead and save this prompt. We can click on the save icon here and this will allow us to save a prompt either privately or for public consumption. Let's go ahead and name this polyprount1. Cool. This takes us directly to the prompt hub. We can see for Polyprount1 that we have a defined chat template and this includes those variables that will be passed in by the user. We also see that we've saved the model configuration here and this includes both the model as well as the temperature and the series of other fields. Finally, we have this code snippet that we can use to pull this prompt directly into our chain code and use it in our application. Let's go ahead and try this out. I'm going to copy this snippet and I'm going to pivot over to a notebook. This notebook should be very similar to what we've looked at previously and essentially the main difference is that the prompt here is going to be pulled directly from the hub. Let's go ahead and run this step. Cool. We can see that the AI message successfully prints out that Poly likes animal crackers. Now let's say and we want to go and iterate on our prompt a little bit. If we go back to the prompt hub and we click edit and playground, we can now edit this prompt. Let's continue on with this toy example where we say you are a French parrot and can only speak French. Now when we commit this new change, we can commit to our same prompt and if we go look at our prompt and we look at our commit history, we can see we've just added a revision.\n",
      "==================================================\n",
      "Chunk 2:\n",
      "In order to use this revision in our code, we can copy this line and this looks the exact same as the line before except we have this commit hash appended to the name of our prompt. Now if we run this cell, we'll respond in French according to the instructions or really is commit. One thing to note is that if we don't provide a commit hash, we will pull the latest version of the prompt every time and to that end we will still respond in French..\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Testing the chunking function\n",
    "chunks = tokenize_and_chunk_hf(loaded_text)\n",
    "\n",
    "# Display the chunks\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(\"All chunks preview:\")\n",
    "print(\"=\"*50)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    print(chunk)\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206cf40e",
   "metadata": {},
   "source": [
    "## Build Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e35053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nada\\AppData\\Local\\Temp\\ipykernel_33496\\2699622044.py:11: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\Nada\\Documents\\AI Engineering bootcamp\\final project\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for each chunk and store them in ChromaDB\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# Initialize a directory to store the database\n",
    "persist_directory = \"chroma_db\"\n",
    "\n",
    "# Initialize the text-to-embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create a new Chroma database or load an existing one\n",
    "# Check if the directory exists, if not create it\n",
    "if not os.path.exists(persist_directory):\n",
    "    os.makedirs(persist_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc13e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nada\\AppData\\Local\\Temp\\ipykernel_33496\\1287538581.py:8: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "# Create a list of documents (chunks) to be stored in the database\n",
    "documents = chunks \n",
    "\n",
    "# Create the Chroma vector store from the documents and embeddings\n",
    "vectorstore = Chroma.from_texts(documents, embedding_model, persist_directory=persist_directory)\n",
    "\n",
    "# Save the vector store to the specified directory\n",
    "vectorstore.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bf1f83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings have been generated and stored successfully in 'chroma_db' directory.\n",
      "Total chunks stored: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embeddings have been generated and stored successfully in '{persist_directory}' directory.\")\n",
    "print(f\"Total chunks stored: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5877d9a",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0040ee4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nada\\AppData\\Local\\Temp\\ipykernel_33496\\2225495744.py:7: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)\n"
     ]
    }
   ],
   "source": [
    "# Searching for the most relevant chunk that answers the user's question using ChromaDB\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Download the database from the specified directory\n",
    "persist_directory = \"chroma_db\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)\n",
    "\n",
    "# Function to search for similar chunks in the database\n",
    "def search_similar_chunks(question, top_k=2):\n",
    "    \"\"\"\n",
    "    Search for similar chunks in the database using Embedding\n",
    "    Args:\n",
    "        question (str): The question to search for\n",
    "        top_k (int): The number of best results to retrieve\n",
    "    Returns:\n",
    "        list: A list of relevant chunks\n",
    "    \"\"\"\n",
    "    # Perform similarity search in the database\n",
    "    results = vectorstore.similarity_search(question, k=top_k)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"Found {len(results)} relevant chunk(s).\")\n",
    "        return [doc.page_content for doc in results]\n",
    "    else:\n",
    "        print(\"No relevant information found for the question.\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d3adae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 relevant chunk(s).\n",
      "\n",
      "Relevant Chunk(s) Retrieved:\n",
      "==================================================\n",
      "Chunk 1:\n",
      "In this video, we're going to be talking about the promptub. The promptub is a feature in Lensmith that allows us to save and version prompts that we've been iterating on and refining. Once we've saved our prompt, we can then also pull it locally into our application and reuse it in our code. To show you how to do this, we're going to continue with our example with our parent named Poly. As a reminder, Poly is a parent and Poly has some facts that she can use to answer user's questions about herself. Let's pretend that we've iterated on this prompt for some time now and we're pretty happy with it and ready to save it for reuse. It's probably not a good idea to save this prompt with these inputs hard-coded in and also with this question from the user hard-coded. What we can do is we can replace these facts with an input variable. This essentially allows the user to, at runtime, pass in the values that they want to use for both the facts that Poly has access to and the question that the human asks. Cool. Now let's go ahead and save this prompt. We can click on the save icon here and this will allow us to save a prompt either privately or for public consumption. Let's go ahead and name this polyprount1. Cool. This takes us directly to the prompt hub. We can see for Polyprount1 that we have a defined chat template and this includes those variables that will be passed in by the user. We also see that we've saved the model configuration here and this includes both the model as well as the temperature and the series of other fields. Finally, we have this code snippet that we can use to pull this prompt directly into our chain code and use it in our application. Let's go ahead and try this out. I'm going to copy this snippet and I'm going to pivot over to a notebook. This notebook should be very similar to what we've looked at previously and essentially the main difference is that the prompt here is going to be pulled directly from the hub. Let's go ahead and run this step. Cool. We can see that the AI message successfully prints out that Poly likes animal crackers. Now let's say and we want to go and iterate on our prompt a little bit. If we go back to the prompt hub and we click edit and playground, we can now edit this prompt. Let's continue on with this toy example where we say you are a French parrot and can only speak French. Now when we commit this new change, we can commit to our same prompt and if we go look at our prompt and we look at our commit history, we can see we've just added a revision.\n",
      "==================================================\n",
      "Chunk 2:\n",
      "In order to use this revision in our code, we can copy this line and this looks the exact same as the line before except we have this commit hash appended to the name of our prompt. Now if we run this cell, we'll respond in French according to the instructions or really is commit. One thing to note is that if we don't provide a commit hash, we will pull the latest version of the prompt every time and to that end we will still respond in French..\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# This part gives us the closest chunk only but does not answer the question\n",
    "# Testing the search function\n",
    "# Taking a question from the user\n",
    "user_question = input(\"Enter your question about the video: \").strip()\n",
    "\n",
    "# Applying the search function to find relevant chunks\n",
    "relevant_chunks = search_similar_chunks(user_question, top_k=2)\n",
    "\n",
    "# Display the relevant chunks\n",
    "if relevant_chunks:\n",
    "    print(\"\\nRelevant Chunk(s) Retrieved:\")\n",
    "    print(\"=\"*50)\n",
    "    for idx, chunk in enumerate(relevant_chunks):\n",
    "        print(f\"Chunk {idx+1}:\\n{chunk}\")\n",
    "        print(\"=\"*50)\n",
    "else:\n",
    "    print(\"No relevant information could be retrieved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc22e70",
   "metadata": {},
   "source": [
    "## Load API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cf76fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4676f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd6f5d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nada\\AppData\\Local\\Temp\\ipykernel_33496\\2744230690.py:3: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the OpenAI model\n",
    "# LangChain \n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name=\"gpt-4\",  \n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48759ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here takes the question and searches for the best chunk and answers it\n",
    "# Retrieval-Augmented Generation (RAG) pipeline\n",
    "# This function will search for the most relevant chunks based on the user's question\n",
    "def rag_qa_pipeline(user_question, top_k=2):\n",
    "    \"\"\"\n",
    "    Applying the smart search and then generating an answer based on the retrieved texts\n",
    "    Args:\n",
    "        user_question (str): user's question\n",
    "        top_k (int): number of best result\n",
    "    Returns:\n",
    "        str: smart answer to the user's question\n",
    "    \"\"\"\n",
    "    # Searching for the most relevant chunks that answer the user's question\n",
    "    relevant_chunks = search_similar_chunks(user_question, top_k=top_k)\n",
    "\n",
    "    if not relevant_chunks:\n",
    "        return \"Sorry, I couldn't find relevant information to answer your question.\"\n",
    "\n",
    "    # Prepare the context for the model (Augmentation)\n",
    "    context_text = \"\\n\\n\".join(relevant_chunks)\n",
    "    prompt_template = \"\"\"\n",
    "    You are an expert assistant. Use the following context to answer the question.\n",
    "    Make sure your answer is in the same language as the question.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question:\n",
    "    {question}\n",
    "    \n",
    "    Answer in a clear, complete, and concise way:\n",
    "    \"\"\"\n",
    "\n",
    "    final_prompt = PromptTemplate.from_template(prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=final_prompt)\n",
    "\n",
    "    # Generate the answer using the LLM\n",
    "    answer = chain.run(context=context_text, question=user_question)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58114c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 relevant chunk(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nada\\AppData\\Local\\Temp\\ipykernel_14376\\3830563266.py:33: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=final_prompt)\n",
      "C:\\Users\\Nada\\AppData\\Local\\Temp\\ipykernel_14376\\3830563266.py:36: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = chain.run(context=context_text, question=user_question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer:\n",
      "==================================================\n",
      "The video is discussing the use of the 'promptub' feature in Lensmith, a tool that allows users to save, version, and reuse prompts that have been refined over time. The tutorial uses an example of a character named Poly to demonstrate how to replace hard-coded inputs with variables for user customization. The video also shows how to save the prompt for private or public use, and how to pull the prompt into the user's code. The tutorial further explains how to make revisions to the saved prompt, such as changing the language of the response, and how these changes are tracked in the commit history.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Taking a question from the user\n",
    "user_question = input(\"Enter your question about the video: \").strip()\n",
    "\n",
    "# Execute the full RAG pipeline\n",
    "final_answer = rag_qa_pipeline(user_question)\n",
    "\n",
    "# Display the final answer\n",
    "print(\"\\nThe question was:\")\n",
    "print(user_question)\n",
    "print(\"\\nThe Answer:\")\n",
    "print(\"=\"*50)\n",
    "print(final_answer)\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176364ff",
   "metadata": {},
   "source": [
    "## RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f028e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 relevant chunk(s).\n",
      "Found 2 relevant chunk(s).\n",
      "Found 3 relevant chunk(s).\n",
      "Found 2 relevant chunk(s).\n",
      "Found 3 relevant chunk(s).\n",
      "Found 2 relevant chunk(s).\n",
      "Found 3 relevant chunk(s).\n",
      "Found 2 relevant chunk(s).\n",
      "\n",
      "All questions answered and contexts retrieved.\n"
     ]
    }
   ],
   "source": [
    "# The whole process: Question ➔ Search ➔ Generate ➔ Direct Evaluation\n",
    "# A list of questions for testing (e.g., the questions we defined earlier)\n",
    "questions = [\n",
    "    \"How can users modify hard-coded inputs in a saved prompt?\",\n",
    "    \"What happens when you save a prompt privately or publicly in the prompt hub?\",\n",
    "    \"How does the system behave if you edit a prompt and commit a new version?\",\n",
    "    \"What effect does not specifying a commit hash have when using a prompt?\"\n",
    "]\n",
    "reference_answers = [\n",
    "    \"Users can modify hard-coded inputs in a saved prompt by replacing these facts with an input variable. This allows the user to pass in the values they want to use for both the facts and the question at runtime. This can be done in the Lensmith's promptub feature where the prompt is saved and versioned. After making these changes, users can save the prompt for reuse.\",\n",
    "    \"Saving a prompt privately or publicly lets users reuse and version prompts via the prompt hub.\",\n",
    "    \"Editing a prompt and committing a new version updates the prompt in the commit history for version control.\",\n",
    "    \"Not specifying a commit hash always pulls the latest version of the prompt.\"\n",
    "]\n",
    "retrieved_contexts_all = []\n",
    "generated_answers_all = []\n",
    "\n",
    "# Retrieval and generation for each question\n",
    "for question in questions:\n",
    "    # Search for the most relevant chunks that answer the user's question\n",
    "    retrieved_chunks = search_similar_chunks(question, top_k=3)\n",
    "    \n",
    "    # Generate the answer using the RAG pipeline\n",
    "    answer = rag_qa_pipeline(question)\n",
    "    \n",
    "    # Save the retrieved chunks and generated answer for each question\n",
    "    retrieved_contexts_all.append(retrieved_chunks)\n",
    "    generated_answers_all.append(answer)\n",
    "\n",
    "print(\"\\nAll questions answered and contexts retrieved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53712089",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5e81a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 12/12 [00:14<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the generated answers using RAGAS\n",
    "import pandas as pd\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision\n",
    "from datasets import Dataset\n",
    "\n",
    "# Prepare the data for evaluation\n",
    "evaluation_dataset = pd.DataFrame({\n",
    "    \"question\": questions,\n",
    "    \"answer\": generated_answers_all,\n",
    "    \"contexts\": retrieved_contexts_all,\n",
    "    \"reference\": reference_answers \n",
    "})\n",
    "\n",
    "# Convert the DataFrame to a Dataset object for RAGAS evaluation\n",
    "evaluation_dataset = Dataset.from_pandas(evaluation_dataset)\n",
    "\n",
    "# Evaluate the generated answers using RAGAS\n",
    "evaluation_result = evaluate(\n",
    "    evaluation_dataset,\n",
    "    metrics=[\n",
    "        faithfulness,        \n",
    "        answer_relevancy,    \n",
    "        context_precision      \n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b6393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation Results:\n",
      "==================================================\n",
      "{'faithfulness': 1.0000, 'answer_relevancy': 0.9749, 'context_precision': 1.0000}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Display the evaluation results\n",
    "print(\"\\nFinal Evaluation Results:\")\n",
    "print(\"=\"*50)\n",
    "print(evaluation_result)\n",
    "print(\"=\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
