{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf23367",
   "metadata": {},
   "source": [
    "# TubeTalk ChatBot \n",
    "YouTube Q&A Bot using Whisper, LangChain, and GPT-4\n",
    "\n",
    "This notebook demonstrates how to build a video understanding chatbot that:\n",
    "- Downloads a YouTube video\n",
    "- Transcribes it with Whisper\n",
    "- Builds a searchable vectorstore\n",
    "- Answers questions using a LangChain agent with GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c49346",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88bece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import uuid\n",
    "import tempfile\n",
    "import asyncio\n",
    "from io import BytesIO\n",
    "import whisper\n",
    "import yt_dlp\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "import chainlit as cl\n",
    "from chainlit.element import Element\n",
    "import numpy as np\n",
    "import soundfile as sf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f263bf07",
   "metadata": {},
   "source": [
    "## Load API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a97ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API keys\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eda1164",
   "metadata": {},
   "source": [
    "## Initialize Models and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77160350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding model\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# Load Whisper model for audio transcription\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "# Initialize global variables\n",
    "session_dir = None\n",
    "vectorstore = None\n",
    "full_transcript = \"\"\n",
    "\n",
    "# Enable LangChain tracing (for debugging/monitoring)\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"TubeTalk bot\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2f13a6",
   "metadata": {},
   "source": [
    "## Session Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd216d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new unique session directory\n",
    "def create_new_session():\n",
    "    global session_dir\n",
    "    session_dir = os.path.join(\"sessions\", str(uuid.uuid4()))\n",
    "    os.makedirs(session_dir, exist_ok=True)\n",
    "\n",
    "# Clean up any existing session resources\n",
    "def clean_session():\n",
    "    global session_dir, vectorstore, full_transcript\n",
    "    # Remove the vectorstore if it exists\n",
    "    if vectorstore is not None:\n",
    "        try:\n",
    "            vectorstore._collection = None\n",
    "            vectorstore = None\n",
    "        except:\n",
    "            pass\n",
    "    # Remove the session directory if it exists\n",
    "    if session_dir and os.path.exists(session_dir):\n",
    "        shutil.rmtree(session_dir)\n",
    "    session_dir = None\n",
    "    full_transcript = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e392d",
   "metadata": {},
   "source": [
    "## YouTube Audio Download and Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca399fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download audio from a YouTube video and return the mp3 path\n",
    "def download_audio_from_youtube(url):\n",
    "    output_template = os.path.join(session_dir, 'audio')\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'outtmpl': output_template,\n",
    "        'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3', 'preferredquality': '192'}],\n",
    "        'quiet': True\n",
    "    }\n",
    "    # Use yt-dlp to download the audio\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.extract_info(url, download=True)\n",
    "    return output_template + \".mp3\"\n",
    "\n",
    "# Transcribe audio file to text using Whisper\n",
    "def transcribe_audio(filepath):\n",
    "    result = whisper_model.transcribe(filepath)\n",
    "    # Extract the text and language from the result\n",
    "    return result[\"text\"], result[\"language\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58f33a",
   "metadata": {},
   "source": [
    "## Tokenization and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e58e137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Tokenize and chunk the text using HuggingFace tokenizer\n",
    "def tokenize_and_chunk_hf(text, max_tokens=256):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    return [tokenizer.decode(tokens[i:i + max_tokens]).strip() for i in range(0, len(tokens), max_tokens)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc158dc",
   "metadata": {},
   "source": [
    "## Build Vectorstore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067156af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and persist a Chroma vectorstore from text chunks\n",
    "def build_vectorstore(chunks):\n",
    "    persist_directory = os.path.join(session_dir, \"chroma_db\")\n",
    "    db = Chroma.from_texts(chunks, embedding_model, persist_directory=persist_directory)\n",
    "    db.persist()\n",
    "    return db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7795fa",
   "metadata": {},
   "source": [
    "## Summarization and Search Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9759614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is called when the user requests a summary of the video transcript.\n",
    "def summarize_text(transcript):\n",
    "    try:\n",
    "        llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-4\", temperature=0.3)\n",
    "        prompt = f\"\"\"\n",
    "        Summarize the following video transcript in a concise and informative way.\n",
    "\n",
    "        Transcript:\n",
    "        {transcript}\n",
    "\n",
    "        Summary:\n",
    "        \"\"\"\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        # Check if the response has content and return it, otherwise return a string representation of the response\n",
    "        return response.content if hasattr(response, \"content\") else str(response)\n",
    "    except Exception as e:\n",
    "        return \"‚ö†Ô∏è Summarization failed: \" + str(e)\n",
    "\n",
    "# This function is called when the user asks a question about the video transcript.\n",
    "def search_transcript(question):\n",
    "    # Search the vectorstore for relevant chunks\n",
    "    vectorstore = cl.user_session.get(\"vectorstore\")\n",
    "    # Check if the vectorstore is available\n",
    "    if vectorstore is None:\n",
    "        return \"‚ùå Vectorstore is not available. Please upload a YouTube video first.\"\n",
    "    # Perform a similarity search to find the most relevant chunks\n",
    "    retrieved_chunks = vectorstore.similarity_search(question, k=3)\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in retrieved_chunks])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eacc7a4",
   "metadata": {},
   "source": [
    "## Create LangChain Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca9196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function initializes the LangChain agent with the summarization and search tools.\n",
    "def build_agent(vectorstore, transcript):\n",
    "    # Wrap the summarize function to use the full transcript from the session\n",
    "    def summarize_wrapper(_):\n",
    "        transcript = cl.user_session.get(\"full_transcript\")\n",
    "        return summarize_text(transcript)\n",
    "    \n",
    "    # Wrap the search function to use the vectorstore from the session\n",
    "    def search_wrapper(question):\n",
    "        if vectorstore is None:\n",
    "            return \"‚ùå Vectorstore is not available. Please upload a YouTube video first.\"\n",
    "        retrieved_chunks = vectorstore.similarity_search(question, k=3)\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in retrieved_chunks])\n",
    "\n",
    "    # Create the tools for the agent\n",
    "    # Summarization tool\n",
    "    summarize_tool = Tool(\n",
    "        name=\"Summarizer\",\n",
    "        func=summarize_wrapper,\n",
    "        description=\"Use this tool to summarize the transcript that extracted from the video in short sentences.\"\n",
    "    )\n",
    "\n",
    "    # Search tool\n",
    "    search_tool = Tool(\n",
    "        name=\"Search\",\n",
    "        func=search_wrapper,\n",
    "        description=\"Use this tool to search for answers from the video transcript,\\\n",
    "            if the question is not related to the video transcript response with:\\\n",
    "            This question isn't related to the video, ask another question.\"\n",
    "    )\n",
    "\n",
    "    # Concatenate the tools into a list\n",
    "    tools = [summarize_tool, search_tool]\n",
    "\n",
    "    # Initialize the agent with the tools and the LLM\n",
    "    agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-4\"),\n",
    "        agent=\"zero-shot-react-description\",\n",
    "        verbose=True\n",
    "    )\n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605409b",
   "metadata": {},
   "source": [
    "## Chainlit Chatbot Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167da73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cl.on_chat_start\n",
    "async def start():\n",
    "    # Clean up any existing session resources\n",
    "    clean_session()             \n",
    "    # Create a new session directory\n",
    "    create_new_session()  \n",
    "\n",
    "    await cl.Message(content=\"üëã Hello! Please enter the YouTube URL to process the video.\").send()\n",
    "    # Set the initial state of the user session\n",
    "    cl.user_session.set(\"state\", \"awaiting_link\")\n",
    "    # Set the initial agent \n",
    "    cl.user_session.set(\"agent\", None)\n",
    "\n",
    "# Handle user messages\n",
    "@cl.on_message\n",
    "async def handle_message(message: cl.Message):\n",
    "    # Get the current state and agent from the user session\n",
    "    state = cl.user_session.get(\"state\")\n",
    "    agent = cl.user_session.get(\"agent\")\n",
    "\n",
    "    if state == \"awaiting_link\":\n",
    "        try:\n",
    "            # Check if the message is a valid YouTube link\n",
    "            if not message.content.startswith(\"https://www.youtube.com/watch?v=\"):\n",
    "                await cl.Message(content=\"üö´ Please enter a valid YouTube link.\").send()\n",
    "                return\n",
    "\n",
    "            url = message.content # Get the YouTube URL from the message\n",
    "            #clean_session()\n",
    "            #create_new_session()\n",
    "\n",
    "            # Download the audio from the YouTube video\n",
    "            await cl.Message(content=\"üîÑ Downloading audio from video...\").send()\n",
    "            audio_file = download_audio_from_youtube(url) \n",
    "\n",
    "            # Transcribe the audio file to text\n",
    "            transcript, _ = await asyncio.to_thread(transcribe_audio, audio_file)\n",
    "            cl.user_session.set(\"full_transcript\", transcript)\n",
    "\n",
    "            # Chunk the transcript into smaller pieces for embedding\n",
    "            chunks = await asyncio.to_thread(tokenize_and_chunk_hf, transcript)\n",
    "\n",
    "            # Create a new Chroma vectorstore and persist it\n",
    "            db = await asyncio.to_thread(build_vectorstore, chunks)\n",
    "            cl.user_session.set(\"vectorstore\", db)\n",
    "\n",
    "            # Create the agent with the vectorstore and transcript\n",
    "            agent = build_agent(db, transcript)\n",
    "            cl.user_session.set(\"agent\", agent)\n",
    "            cl.user_session.set(\"state\", \"ready_for_questions\")\n",
    "\n",
    "            # Send a message to the user indicating that the video has been processed\n",
    "            await cl.Message(content=\"‚úÖ Video processed! Ask your question by text or voice.\").send()\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle any exceptions that occur during the download or transcription process\n",
    "            await cl.Message(content=f\"‚ùå Error processing video: {str(e)}\").send()\n",
    "\n",
    "    elif state == \"ready_for_questions\":\n",
    "        try:\n",
    "            await cl.Message(content=\"ü§î Thinking...\").send()\n",
    "            # Use the agent to answer the question\n",
    "            response = agent.invoke(message.content) \n",
    "\n",
    "            # Check if the response is a string or a dictionary and handle accordingly\n",
    "            if isinstance(response, dict) and \"output\" in response:\n",
    "                await cl.Message(content=response[\"output\"]).send()\n",
    "            elif hasattr(response, \"content\"):\n",
    "                await cl.Message(content=response.content).send()\n",
    "            else:\n",
    "                await cl.Message(content=str(response)).send()\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle any exceptions that occur during the agent invocation\n",
    "            await cl.Message(content=f\"‚ö†Ô∏è Error answering: {str(e)}\").send()\n",
    "    else:\n",
    "        # If the state is not recognized, prompt the user to enter a YouTube link\n",
    "        await cl.Message(content=\"üö´ Please enter a YouTube link first.\").send()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125f1a4",
   "metadata": {},
   "source": [
    "## Voice Input Handling Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cl.on_audio_start\n",
    "# This function is called when the audio recording starts\n",
    "async def on_audio_start():\n",
    "    await cl.Message(content=\"üé§ Start recording...\").send()\n",
    "    return True\n",
    "\n",
    "\n",
    "@cl.on_audio_chunk\n",
    "# This function is called when an audio chunk is received\n",
    "async def on_audio_chunk(chunk: cl.InputAudioChunk):\n",
    "    # Check if the chunk is the start of a new audio recording\n",
    "    if chunk.isStart:\n",
    "        buffer = BytesIO()\n",
    "        extension = \"wav\"\n",
    "        # Check if the chunk has a mimeType and extract the extension from it\n",
    "        # If the mimeType is not empty and contains a \"/\", split it to get the extension\n",
    "        if chunk.mimeType and \"/\" in chunk.mimeType:\n",
    "            parts = chunk.mimeType.split(\"/\")\n",
    "            # Check if the mimeType is \"audio/wav\" or \"audio/mp3\" and set the extension accordingly\n",
    "            if len(parts) > 1: # Check if there are at least two parts after splitting\n",
    "                extension = parts[1]\n",
    "        buffer.name = f\"input_audio.{extension}\"\n",
    "        # Set the buffer and mimeType in the user session\n",
    "        # Store the buffer in the user session for later use\n",
    "        cl.user_session.set(\"audio_buffer\", buffer)\n",
    "        cl.user_session.set(\"audio_mime_type\", chunk.mimeType)\n",
    "\n",
    "    # Check if the chunk is not the start of a new audio recording\n",
    "    buffer = cl.user_session.get(\"audio_buffer\")\n",
    "    if buffer: # If the buffer exists in the user session\n",
    "        # Write the audio data to the buffer\n",
    "        buffer.write(chunk.data)\n",
    "\n",
    "@cl.on_audio_end\n",
    "# This function is called when the audio recording ends\n",
    "async def on_audio_end(elements: list[Element] = None):\n",
    "    elements = elements or [] # Default to an empty list if no elements are provided\n",
    "\n",
    "    audio_buffer: BytesIO = cl.user_session.get(\"audio_buffer\")\n",
    "    if not audio_buffer: # If the audio buffer is not available in the user session\n",
    "        await cl.Message(content=\"‚ö†Ô∏è Doesn't recording anything, Please try again \").send()\n",
    "        return\n",
    "\n",
    "    # Reset the buffer position to the beginning\n",
    "    # Read the audio data from the buffer and set the mimeType in the user session\n",
    "    # Set the mimeType in the user session for later use\n",
    "    audio_buffer.seek(0)\n",
    "    audio_file = audio_buffer.read()\n",
    "    audio_mime_type: str = cl.user_session.get(\"audio_mime_type\") or \"audio/wav\"\n",
    "\n",
    "    # Create a tuple with the audio file name, bytes, and mimeType\n",
    "    whisper_input = (audio_buffer.name, audio_file, audio_mime_type)\n",
    "    # Transcribe the audio using Whisper     \n",
    "    transcription = await speech_to_text(whisper_input)\n",
    "\n",
    "    # Display the transcription in the chat\n",
    "    await cl.Message(\n",
    "        author=\"You\",\n",
    "        type=\"user_message\",\n",
    "        content=transcription\n",
    "    ).send()\n",
    "\n",
    "    # Set the state in the user session to indicate that the user can ask questions\n",
    "    cl.user_session.set(\"state\", \"ready_for_questions\")\n",
    "\n",
    "    # Create a new message object with the transcription\n",
    "    msg = cl.Message(author=\"You\", content=transcription, elements=[]) \n",
    "    # Set the message type to \"user_message\" and add it to the elements list\n",
    "    await handle_message(message=msg) \n",
    "\n",
    "# This function is called to transcribe the audio using Whisper\n",
    "async def speech_to_text(whisper_input):\n",
    "    file_name, file_bytes, mime_type = whisper_input\n",
    "\n",
    "    os.makedirs(\"saved_audio\", exist_ok=True) # Create the directory if it doesn't exist\n",
    "    # Generate a temporary file name for the audio file\n",
    "    temp_path = f\"saved_audio/{file_name}\"\n",
    "\n",
    "    # Write the audio bytes to a temporary file\n",
    "    audio_data = np.frombuffer(file_bytes, dtype=np.int16)\n",
    "    sf.write(temp_path, audio_data, samplerate=16000, format='WAV', subtype='PCM_16')\n",
    "\n",
    "    # Transcribe the audio file using Whisper\n",
    "    vc_transcript = whisper_model.transcribe(temp_path)[\"text\"]\n",
    "\n",
    "    return vc_transcript"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
